{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Bhd7pYbkf-",
        "outputId": "c86e86e0-df20-4007-8bee-2abee6aae9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install groq langchain ipywidgets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "from google.colab import userdata, files\n",
        "from groq import Groq\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,      # For Fixed Size Chunking\n",
        "    SentenceTransformersTokenTextSplitter, # For Sentence Chunking (Alternative: NLTKSentenceSplitter)\n",
        "    RecursiveCharacterTextSplitter # For Recursive Chunking\n",
        ")\n",
        "\n",
        "# --- Initialize Groq Client ---\n",
        "# Make sure you have stored your Groq API key in Colab Secrets named 'GROQ_API_KEY'\n",
        "try:\n",
        "    api_key = userdata.get('GROQ_API_KEY')\n",
        "    client = Groq(api_key=api_key)\n",
        "    print(\"Groq API key loaded successfully.\")\n",
        "    # You can print the model name if needed, or define it here\n",
        "    model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\" # Example from your doc\n",
        "except Exception as e:\n",
        "    print(f\" Error loading API key: {e}\")\n",
        "    # Handle error or stop execution if key is critical\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWQN8BvhcCrE",
        "outputId": "8ffb6a31-4e51-40dd-cf3b-1b02ed98eddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq API key loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your 5-page text document (.txt)\n",
        "print(\"Please upload your 5-page text document (.txt):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process the uploaded file\n",
        "document_text = \"\"\n",
        "filename = \"\"\n",
        "if len(uploaded) == 1:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    document_text = uploaded[filename].decode('utf-8')\n",
        "    print(f\"Loaded document: {filename}\")\n",
        "    print(f\"   Approximate words: {len(document_text.split())}\")\n",
        "elif len(uploaded) == 0:\n",
        "    print(\"No file uploaded.\")\n",
        "else:\n",
        "    print(\"Please upload exactly one file.\")\n",
        "\n",
        "# Check if document was loaded\n",
        "if not document_text:\n",
        "    print(\" No document text available. Please upload a file and rerun this cell.\")\n",
        "else:\n",
        "    # Optional: Basic validation or preview\n",
        "    # print(f\"First 500 characters:\\n{document_text[:500]}...\")\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "itBgXHD-cG2w",
        "outputId": "76eb0a36-6e5f-4a9a-a3c8-0b358a1a7718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your 5-page text document (.txt):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a29ccde6-b29a-4758-a34c-e77fa053126b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a29ccde6-b29a-4758-a34c-e77fa053126b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving International Journal of Women's De.txt to International Journal of Women's De (1).txt\n",
            "Loaded document: International Journal of Women's De (1).txt\n",
            "   Approximate words: 3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Chunking Parameters ---\n",
        "# You can adjust these parameters based on your document and requirements\n",
        "\n",
        "# Parameters for Fixed Size (Character) Chunking\n",
        "FIXED_CHUNK_SIZE = 1000  # Characters\n",
        "FIXED_CHUNK_OVERLAP = 100 # Characters\n",
        "\n",
        "# Parameters for Sentence Chunking (using Token-based approx)\n",
        "# Note: SentenceTransformersTokenTextSplitter splits by sentences first, then ensures chunks are under token limit.\n",
        "SENTENCE_CHUNK_SIZE = 80  # Tokens (approx sentence length limit) 50 to 60 chara max\n",
        "SENTENCE_CHUNK_OVERLAP = 10 # Tokens\n",
        "\n",
        "# Parameters for Recursive Character Chunking\n",
        "RECURSIVE_CHUNK_SIZE = 1000  # Characters\n",
        "RECURSIVE_CHUNK_OVERLAP = 100 # Characters\n",
        "\n",
        "# --- Initialize Chunking Splitters ---\n",
        "try:\n",
        "    fixed_size_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\\n\",  # Try splitting by paragraphs first\n",
        "        chunk_size=FIXED_CHUNK_SIZE,\n",
        "        chunk_overlap=FIXED_CHUNK_OVERLAP,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    print(\"Fixed Size Chunking Splitter initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\" Error initializing Fixed Size Splitter: {e}\")\n",
        "    fixed_size_splitter = None\n",
        "\n",
        "try:\n",
        "    # Requires sentence-transformers: !pip install sentence-transformers\n",
        "    # This splits into sentences, then groups sentences into chunks up to chunk_size tokens.\n",
        "    sentence_splitter = SentenceTransformersTokenTextSplitter(\n",
        "        chunk_size=SENTENCE_CHUNK_SIZE,\n",
        "        chunk_overlap=SENTENCE_CHUNK_OVERLAP\n",
        "    )\n",
        "    print(\" Sentence Chunking Splitter initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Sentence Splitter (might need 'sentence-transformers'): {e}\")\n",
        "    print(\"   Trying alternative NLTK-based sentence splitter...\")\n",
        "    try:\n",
        "        # Alternative if sentence-transformers is problematic\n",
        "        from langchain.text_splitter import NLTKTextSplitter\n",
        "        sentence_splitter = NLTKTextSplitter() # Splits by sentences\n",
        "        # Wrap it to add chunking if needed\n",
        "        sentence_splitter = CharacterTextSplitter(\n",
        "             separator=\" \", # Split sentences by space if needed, not ideal but simpler\n",
        "             chunk_size=SENTENCE_CHUNK_SIZE * 4, # Approx chars, very rough\n",
        "             chunk_overlap=SENTENCE_CHUNK_OVERLAP * 4,\n",
        "             length_function=len\n",
        "        )\n",
        "        print(\" Alternative Sentence Chunking Splitter initialized.\")\n",
        "    except Exception as alt_e:\n",
        "        print(f\"Error initializing alternative Sentence Splitter: {alt_e}\")\n",
        "        sentence_splitter = None\n",
        "\n",
        "\n",
        "try:\n",
        "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=RECURSIVE_CHUNK_SIZE,\n",
        "        chunk_overlap=RECURSIVE_CHUNK_OVERLAP,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Try these separators in order\n",
        "    )\n",
        "    print(\"Recursive Character Chunking Splitter initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\" Error initializing Recursive Splitter: {e}\")\n",
        "    recursive_splitter = None\n",
        "\n",
        "# Store splitters for easy access\n",
        "splitters = {\n",
        "    \"Fixed Size\": fixed_size_splitter,\n",
        "    \"Sentence\": sentence_splitter,\n",
        "    \"Recursive\": recursive_splitter\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLLN4xjcVNc",
        "outputId": "1b9cc24e-d5e7-44a8-c671-267f033c657b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Size Chunking Splitter initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentence Chunking Splitter initialized.\n",
            "Recursive Character Chunking Splitter initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if document is loaded\n",
        "if not document_text:\n",
        "    print(\" No document text available. Please run the upload cell first.\")\n",
        "else:\n",
        "    chunked_results = {}\n",
        "\n",
        "    for strategy_name, splitter in splitters.items():\n",
        "        print(f\"\\n--- Applying Chunking Strategy: {strategy_name} ---\")\n",
        "        if splitter is None:\n",
        "            print(f\" Splitter for {strategy_name} not initialized. Skipping.\")\n",
        "            chunked_results[strategy_name] = {'error': 'Splitter not initialized'}\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Perform the chunking\n",
        "            docs = splitter.create_documents([document_text])\n",
        "\n",
        "            print(f\"Number of chunks created: {len(docs)}\")\n",
        "            # Optional: Print first chunk to inspect\n",
        "            # if docs:\n",
        "            #     print(f\"Sample Chunk 1 (chars {len(docs[0].page_content)}):\\n{docs[0].page_content[:200]}...\")\n",
        "\n",
        "            # Store results\n",
        "            chunked_results[strategy_name] = {\n",
        "                'chunks': docs,\n",
        "                'num_chunks': len(docs)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error applying {strategy_name} chunking: {e}\")\n",
        "            chunked_results[strategy_name] = {'error': str(e)}\n",
        "\n",
        "    print(\"\\n--- Chunking Process Complete ---\")\n",
        "    # chunked_results now contains the output for each strategy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TKjZW4zd3_s",
        "outputId": "76895aa2-94f7-4599-9734-07245ae37f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Chunking Strategy: Fixed Size ---\n",
            "Number of chunks created: 1\n",
            "\n",
            "--- Applying Chunking Strategy: Sentence ---\n",
            "Number of chunks created: 18\n",
            "\n",
            "--- Applying Chunking Strategy: Recursive ---\n",
            "Number of chunks created: 31\n",
            "\n",
            "--- Chunking Process Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Import necessary libraries (if not already imported) ---\n",
        "import re # For basic text search\n",
        "\n",
        "# --- Define User Query Functions (Text-Based, No Embeddings) ---\n",
        "# These functions will work directly with the chunked text data.\n",
        "\n",
        "def query_find_keyword(query_text, chunks_data):\n",
        "    \"\"\"\n",
        "    Query Function 1: Find chunks containing specific keywords.\n",
        "    Strategy: Search chunk text for the keywords present in the query.\n",
        "    \"\"\"\n",
        "    print(f\"--- Query (Keyword Search): {query_text} ---\")\n",
        "    # Extract keywords from the query (simple split, could be improved)\n",
        "    keywords = query_text.lower().split()\n",
        "    # Remove common stop words if desired (optional)\n",
        "    # stop_words = {'find', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can'}\n",
        "    # keywords = [word for word in keywords if word not in stop_words]\n",
        "\n",
        "    if not keywords:\n",
        "        print(\"No keywords found in query.\")\n",
        "        return\n",
        "\n",
        "    chunk_texts = [doc.page_content for doc in chunks_data['chunks']]\n",
        "    if not chunk_texts:\n",
        "        print(\"No chunks available for this method.\")\n",
        "        return\n",
        "\n",
        "    matching_chunks = []\n",
        "    for i, chunk_text in enumerate(chunk_texts):\n",
        "        # Simple case-insensitive search for any keyword\n",
        "        # This could be made more sophisticated (e.g., phrase search, regex)\n",
        "        found_keywords = [kw for kw in keywords if kw in chunk_text.lower()]\n",
        "        if found_keywords:\n",
        "            matching_chunks.append((i, chunk_text, found_keywords))\n",
        "\n",
        "    if matching_chunks:\n",
        "        print(f\"Found {len(matching_chunks)} chunk(s) containing keywords: {', '.join(keywords)}\")\n",
        "        for idx, chunk_text, found_kws in matching_chunks:\n",
        "            print(f\"\\n--- Match in Chunk {idx} (Keywords found: {', '.join(found_kws)}) ---\")\n",
        "            # Print a snippet around the first found keyword for context\n",
        "            first_kw = found_kws[0]\n",
        "            # Find the position of the keyword (case-insensitive)\n",
        "            pos = chunk_text.lower().find(first_kw)\n",
        "            start = max(0, pos - 100) # Show 100 chars before\n",
        "            end = min(len(chunk_text), pos + len(first_kw) + 100) # Show 100 chars after\n",
        "            snippet = chunk_text[start:end]\n",
        "            # Highlight the keyword in the snippet (optional)\n",
        "            # snippet = re.sub(f'({re.escape(first_kw)})', r'**\\1**', snippet, flags=re.IGNORECASE)\n",
        "            print(f\"...{snippet}...\")\n",
        "    else:\n",
        "        print(f\"No chunks found containing keywords: {', '.join(keywords)}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "def query_get_chunk_stats(query_text, chunks_data):\n",
        "    \"\"\"\n",
        "    Query Function 2: Get statistics about the chunks.\n",
        "    Strategy: Calculate number of chunks, average length, etc.\n",
        "    \"\"\"\n",
        "    print(f\"--- Query (Chunk Statistics): {query_text} ---\")\n",
        "    try:\n",
        "        num_chunks = chunks_data.get('num_chunks', len(chunks_data.get('chunks', [])))\n",
        "        chunks_list = chunks_data.get('chunks', [])\n",
        "        if not chunks_list:\n",
        "            print(\"No chunks available.\")\n",
        "            return\n",
        "\n",
        "        lengths = [len(doc.page_content) for doc in chunks_list]\n",
        "        total_chars = sum(lengths)\n",
        "        avg_chars = total_chars / len(lengths) if lengths else 0\n",
        "        min_chars = min(lengths) if lengths else 0\n",
        "        max_chars = max(lengths) if lengths else 0\n",
        "\n",
        "        print(f\"Number of Chunks: {num_chunks}\")\n",
        "        print(f\"Total Characters: {total_chars}\")\n",
        "        print(f\"Average Chunk Length (chars): {avg_chars:.2f}\")\n",
        "        print(f\"Shortest Chunk Length (chars): {min_chars}\")\n",
        "        print(f\"Longest Chunk Length (chars): {max_chars}\")\n",
        "        print(\"-\" * 40)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating statistics: {e}\")\n",
        "\n",
        "def query_preview_chunks(query_text, chunks_data):\n",
        "    \"\"\"\n",
        "    Query Function 3: Preview the beginning and end chunks.\n",
        "    Strategy: Show the first and last few chunks.\n",
        "    \"\"\"\n",
        "    print(f\"--- Query (Preview Chunks): {query_text} ---\")\n",
        "    try:\n",
        "        chunks_list = chunks_data.get('chunks', [])\n",
        "        if not chunks_list:\n",
        "            print(\"No chunks available.\")\n",
        "            return\n",
        "        num_chunks = len(chunks_list)\n",
        "        print(f\"Total Chunks: {num_chunks}\")\n",
        "\n",
        "        n_preview = min(2, num_chunks) # Preview first and last 2, or less if fewer chunks\n",
        "        print(f\"\\n--- First {n_preview} Chunk(s) ---\")\n",
        "        for i in range(n_preview):\n",
        "            print(f\"\\n--- Chunk {i} (Length: {len(chunks_list[i].page_content)} chars) ---\")\n",
        "            content = chunks_list[i].page_content\n",
        "            print(content[:500] + (\"...\" if len(content) > 500 else \"\"))\n",
        "\n",
        "        if num_chunks > n_preview:\n",
        "            print(f\"\\n--- Last {n_preview} Chunk(s) ---\")\n",
        "            for i in range(max(n_preview, num_chunks - n_preview), num_chunks):\n",
        "                 print(f\"\\n--- Chunk {i} (Length: {len(chunks_list[i].page_content)} chars) ---\")\n",
        "                 content = chunks_list[i].page_content\n",
        "                 print(content[:500] + (\"...\" if len(content) > 500 else \"\"))\n",
        "        print(\"-\" * 40)\n",
        "    except Exception as e:\n",
        "        print(f\"Error previewing chunks: {e}\")\n",
        "\n",
        "def query_find_longest_shortest(query_text, chunks_data):\n",
        "    \"\"\"\n",
        "    Query Function 4: Find the longest and shortest chunks.\n",
        "    Strategy: Iterate through chunks and compare lengths.\n",
        "    \"\"\"\n",
        "    print(f\"--- Query (Find Longest/Shortest): {query_text} ---\")\n",
        "    try:\n",
        "        chunks_list = chunks_data.get('chunks', [])\n",
        "        if not chunks_list:\n",
        "            print(\"No chunks available.\")\n",
        "            return\n",
        "\n",
        "        lengths = [(i, len(doc.page_content)) for i, doc in enumerate(chunks_list)]\n",
        "        if not lengths:\n",
        "             print(\"No chunk lengths calculated.\")\n",
        "             return\n",
        "\n",
        "        # Find shortest and longest\n",
        "        shortest_idx, shortest_len = min(lengths, key=lambda x: x[1])\n",
        "        longest_idx, longest_len = max(lengths, key=lambda x: x[1])\n",
        "\n",
        "        print(f\"Shortest Chunk: Index {shortest_idx}, Length {shortest_len} chars\")\n",
        "        print(f\"Content (first 300 chars): {chunks_list[shortest_idx].page_content[:300]}...\")\n",
        "        print(\"-\" * 20)\n",
        "        print(f\"Longest Chunk: Index {longest_idx}, Length {longest_len} chars\")\n",
        "        print(f\"Content (first 300 chars): {chunks_list[longest_idx].page_content[:300]}...\")\n",
        "        print(\"-\" * 40)\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding longest/shortest chunks: {e}\")\n",
        "\n",
        "def query_simple_summary_by_concat(query_text, chunks_data):\n",
        "    \"\"\"\n",
        "    Query Function 5: Simple summary by concatenating beginnings of chunks.\n",
        "    Strategy: Take the first N characters from the first M chunks.\n",
        "    Note: This is a very basic approach, not semantic like LLM summaries.\n",
        "    \"\"\"\n",
        "    print(f\"--- Query (Simple Concat Summary): {query_text} ---\")\n",
        "    try:\n",
        "        chunks_list = chunks_data.get('chunks', [])\n",
        "        if not chunks_list:\n",
        "            print(\"No chunks available.\")\n",
        "            return\n",
        "\n",
        "        summary_parts = []\n",
        "        chars_taken_total = 0\n",
        "        target_summary_length = 1000 # Aim for a summary of ~1000 chars\n",
        "        chars_per_chunk = 200 # Take first 200 chars from each chunk initially\n",
        "\n",
        "        for doc in chunks_list:\n",
        "            content = doc.page_content\n",
        "            take_chars = min(chars_per_chunk, len(content), target_summary_length - chars_taken_total)\n",
        "            if take_chars > 0:\n",
        "                summary_parts.append(content[:take_chars])\n",
        "                chars_taken_total += take_chars\n",
        "            if chars_taken_total >= target_summary_length:\n",
        "                break\n",
        "\n",
        "        if summary_parts:\n",
        "            combined_summary = \" ... \".join(summary_parts) # Join with separator\n",
        "            print(f\"Combined Summary (approx {chars_taken_total} chars):\")\n",
        "            print(combined_summary)\n",
        "        else:\n",
        "            print(\"Could not generate summary from chunks.\")\n",
        "        print(\"-\" * 40)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating simple summary: {e}\")\n",
        "\n",
        "# --- Example Usage of Query Functions ---\n",
        "# You can run these queries after chunking is complete (Cell 5) and before embedding (Cell 6+)\n",
        "\n",
        "# Let's run the queries on one of the chunking results, e.g., 'Recursive'\n",
        "target_method_for_queries = \"Recursive\" # You can change this to \"Fixed Size\" or \"Sentence\"\n",
        "\n",
        "if target_method_for_queries in chunked_results and 'error' not in chunked_results[target_method_for_queries]:\n",
        "    chunks_data_for_query = chunked_results[target_method_for_queries]\n",
        "\n",
        "    # Define your simple text-based queries\n",
        "    simple_queries_and_functions = [\n",
        "        (\"Find information about clinical trials\", query_find_keyword),\n",
        "        (\"Show me statistics about the chunks\", query_get_chunk_stats),\n",
        "        (\"Preview the first and last chunks\", query_preview_chunks),\n",
        "        (\"Which chunks are the longest and shortest?\", query_find_longest_shortest),\n",
        "        (\"Create a simple summary from chunk beginnings\", query_simple_summary_by_concat),\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nRunning simple text-based queries using chunks from '{target_method_for_queries}' method:\")\n",
        "    print(\"=\"*70)\n",
        "    # Execute queries\n",
        "    for query_text, query_func in simple_queries_and_functions:\n",
        "        query_func(query_text, chunks_data_for_query)\n",
        "        print(\"\\n\" + \"=\"*70 + \"\\n\") # Separator between query results\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Chunking results for method '{target_method_for_queries}' not found or has errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YvJkGfevEJL",
        "outputId": "04bcbc24-cb36-476e-8b25-01551628ba77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running simple text-based queries using chunks from 'Recursive' method:\n",
            "======================================================================\n",
            "--- Query (Keyword Search): Find information about clinical trials ---\n",
            "Found 30 chunk(s) containing keywords: find, information, about, clinical, trials\n",
            "\n",
            "--- Match in Chunk 0 (Keywords found: clinical, trials) ---\n",
            "...–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Dermatology\r\n",
            " Clinical trials in dermatology\r\n",
            " K. Torre, M. Shahriari⁎\r\n",
            " Department of Dermatology, University of Connecti...\n",
            "\n",
            "--- Match in Chunk 1 (Keywords found: clinical, trials) ---\n",
            "...videtreatmentsthatnot onlyofferobjectiveimprovementsinclinicaldiseasestatus butalsosubjective im\r\n",
            "provementsinthequalityoflifeofpatients whoare afflictedwith thed...\n",
            "\n",
            "--- Match in Chunk 2 (Keywords found: clinical) ---\n",
            "...c health (Bhatt, 2010; Collier, 2009).\r\n",
            " Dr. James Lind is often credited with conducting the first clinical\r\n",
            " trial in modern times (Bhatt, 2010; Collier, 2009). Although the\r\n",
            " study was notperfect, heimplem...\n",
            "\n",
            "--- Match in Chunk 3 (Keywords found: clinical, trials) ---\n",
            "...prevent scurvy (Bartholomew, 2002). By 1863, another element of\r\n",
            " modern clinical trial protocols, the placebo, was added to research\r\n",
            " studies (de Craen et al., 1999). In the1900s,...\n",
            "\n",
            "--- Match in Chunk 4 (Keywords found: clinical, trials) ---\n",
            "...mentation. The United States also imple\r\n",
            "mented its own laws and regulatory organizations to govern clinical\r\n",
            " trial research; however, many years, legislative acts, and unfortunate\r\n",
            " deathspassedbefore theU....\n",
            "\n",
            "--- Match in Chunk 5 (Keywords found: clinical, trials) ---\n",
            "...itions, diabetes, cancer, and an array of diseases that\r\n",
            " affect patients worldwide.\r\n",
            " Results from clinical trials have significantly improved the out\r\n",
            "comes and quality of life of patients with dermatologic...\n",
            "\n",
            "--- Match in Chunk 6 (Keywords found: clinical) ---\n",
            "...he early\r\n",
            " 2000s, targeted immunotherapy for patients with psoriasis has be\r\n",
            "come a strong focus in clinical research and resulted in the develop\r\n",
            "ment of new therapeutics on a yearly basis and half a dozen a...\n",
            "\n",
            "--- Match in Chunk 7 (Keywords found: clinical, trials) ---\n",
            "...tis, hypertension, heart dis\r\n",
            "ease, diabetes, and depression.Measures of treatment efficacywithin\r\n",
            " clinical trials should not only focus on clinical improvement but also\r\n",
            " incorporate patients' perceptions o...\n",
            "\n",
            "--- Match in Chunk 8 (Keywords found: clinical, trials) ---\n",
            "...uct and assess the long-term adverse events and effects in vary\r\n",
            "ing populations (Collier, 2009).\r\n",
            " Clinical trials can be conducted at academic medical centers and\r\n",
            " in private practice settings. Typically, ...\n",
            "\n",
            "--- Match in Chunk 9 (Keywords found: information) ---\n",
            "...trial can be conducted at the PI's site. The questionnaire\r\n",
            " surveys the investigator's experience, information on thesite and pa\r\n",
            "tient population, and provides the PI with the full study protocol to\r\n",
            " determin...\n",
            "\n",
            "--- Match in Chunk 10 (Keywords found: information, about) ---\n",
            "...with use of the facility, available resources, and support staff. After\r\n",
            " this information is collected, sponsor representatives conduct a pre\r\n",
            "liminary onsite visit to confirm that the site...\n",
            "\n",
            "--- Match in Chunk 11 (Keywords found: information, about) ---\n",
            "...ormed consent documents are also critical to initiate a study\r\n",
            " and provide study subjects with the information they need to make...\n",
            "\n",
            "--- Match in Chunk 12 (Keywords found: find, information, about, clinical, trials) ---\n",
            "...\r\n",
            "tion for the study in question. Some academic centers rely on a\r\n",
            " patient registry or database to find potential study subjects (Bain,...\n",
            "\n",
            "--- Match in Chunk 13 (Keywords found: find, clinical, trials) ---\n",
            "...patient registry or database to find potential study subjects (Bain,\r\n",
            " 2005) whereas others use broadcast messaging techniques, flyers,\r...\n",
            "\n",
            "--- Match in Chunk 14 (Keywords found: clinical, trials) ---\n",
            "...versed in all aspects of the study protocol,\r\n",
            " initiate the recruitment of patients, and ensure the clinical care of\r\n",
            " all subjects. They also record important data from the study subjects\r\n",
            " in case report fo...\n",
            "\n",
            "--- Match in Chunk 15 (Keywords found: clinical, trials) ---\n",
            "... with\r\n",
            "out coordinators, there would be no study patients. They are an in\r\n",
            "valuable resource to the clinical trial team.\r\n",
            " Please cite this article as: Torre K, Shahriari M, Clinical trials in dermatology, In...\n",
            "\n",
            "--- Match in Chunk 17 (Keywords found: information, about, clinical, trials) ---\n",
            "...cardiograms, etc.), uncertainty about the treatment and possible\r\n",
            " side effects, and concerns about information in the consent paper\r\n",
            "work (Ross et al., 1999). Experienced study personnel can help ad\r\n",
            "dress the ...\n",
            "\n",
            "--- Match in Chunk 18 (Keywords found: information, clinical) ---\n",
            "...correct information.\r\n",
            " Randomized, controlled, double-blinded studies aim to eliminate\r\n",
            " bias and contribute valid stat...\n",
            "\n",
            "--- Match in Chunk 19 (Keywords found: clinical, trials) ---\n",
            "...es of Health have established ini\r\n",
            "tiatives to increase the enrollment of women and minorities in\r\n",
            " clinical trials.\r\n",
            " One problem that is frequently noted in clinical literature is the\r\n",
            " potential for public...\n",
            "\n",
            "--- Match in Chunk 20 (Keywords found: clinical, trials) ---\n",
            "...icians to not only be aware of positive results but\r\n",
            " The final challenge is the application of the clinical trial results to\r\n",
            " one's ownpractice.Aclinician needstocarefully assesswhethertheir\r\n",
            " patient is ad...\n",
            "\n",
            "--- Match in Chunk 21 (Keywords found: clinical, trials) ---\n",
            "...nique to each patient be developed.\r\n",
            " Conclusion\r\n",
            " It is imperative that wecontinuetoengageinqualityclinical trials\r\n",
            " to support advances in science and medicine. As Ellimoottil et al.\r\n",
            " (2015) state, “clinic...\n",
            "\n",
            "--- Match in Chunk 22 (Keywords found: clinical, trials) ---\n",
            "...ffective future treat\r\n",
            "ments (Campa et al., 2016). With the recent encouraging results of\r\n",
            " multipleclinical trials of patients withpsoriasiswhoshowsubstantial\r\n",
            " Psoriasis Area and Severity Index score improv...\n",
            "\n",
            "--- Match in Chunk 23 (Keywords found: clinical, trials) ---\n",
            "...withproperlydesignedclinicaltrials,this canandwill beconsistently\r\n",
            " achieved worldwide.\r\n",
            " References\r\n",
            " Aitken LM, Pelter MM,Carl...\n",
            "\n",
            "--- Match in Chunk 24 (Keywords found: clinical, trials) ---\n",
            "...nter A. Developing more open and equitable relationships with\r\n",
            " industry to improve advancements in clinical research in dermatology. Br J\r\n",
            " Dermatol 2016;174:1365–9.\r\n",
            " Carson PA. Clinical research in dermato...\n",
            "\n",
            "--- Match in Chunk 25 (Keywords found: clinical, trials) ---\n",
            "...2003;120:932–41.\r\n",
            " Please cite this article as: Torre K, Shahriari M, Clinical trials in dermatology, International Journal of Women's Dermatology (2016), http://\r\n",
            " dx.doi.org/10...\n",
            "\n",
            "--- Match in Chunk 26 (Keywords found: clinical, trials) ---\n",
            "...peutic agents, 2005-2012. JAMA 2014;\r\n",
            " 311:368–77.\r\n",
            " Ellimoottil C, Vijan S, Flanigan RC. A primeronclinical trial design. Urol Oncol 2015;33:\r\n",
            " 116–21.\r\n",
            " Fahey T. Applying the results of clinical trials to p...\n",
            "\n",
            "--- Match in Chunk 27 (Keywords found: clinical, trials) ---\n",
            "...eczema: a systematic review. Allergy 2017;72:146–63.\r\n",
            " Green L. Explaining the role of the nurse in clinical trials. Nurs Stand 2011;25:35–9.\r\n",
            " Hastings CE, Fisher CA, McCabe MA, National Clinical Research Nu...\n",
            "\n",
            "--- Match in Chunk 28 (Keywords found: clinical, trials) ---\n",
            "...cally significant results. Eur J Cancer 2007;43:2559–79.\r\n",
            " Liu KA, Mager NA. Women's involvement in clinical trials: historical perspective and\r\n",
            " future implications. Pharm Pract (Granada) 2016;14:708–16.\r\n",
            " M...\n",
            "\n",
            "--- Match in Chunk 29 (Keywords found: clinical, trials) ---\n",
            "...999;52:\r\n",
            " 1143–56.\r\n",
            " Sadler GR, Lantz JM, Fullerton JT, Dault Y. Nurses' unique roles in randomized clinical\r\n",
            " trials. J Prof Nurs 1999;15:106–15.\r\n",
            " Schulz KF, Chalmers I, Altman DG. The landscape and lexicon...\n",
            "\n",
            "--- Match in Chunk 30 (Keywords found: clinical, trials) ---\n",
            "...anization. Am J Gastroenterol 2007;102:\r\n",
            " 1429–35.\r\n",
            " Zarbin MA.Challenges inapplying the results of clinical trials to clinical practice. JAMA\r\n",
            " Ophthalmol 2016;134:928–33.\r\n",
            " Please cite this article as: Torr...\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- Query (Chunk Statistics): Show me statistics about the chunks ---\n",
            "Number of Chunks: 31\n",
            "Total Characters: 29363\n",
            "Average Chunk Length (chars): 947.19\n",
            "Shortest Chunk Length (chars): 543\n",
            "Longest Chunk Length (chars): 997\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- Query (Preview Chunks): Preview the first and last chunks ---\n",
            "Total Chunks: 31\n",
            "\n",
            "--- First 2 Chunk(s) ---\n",
            "\n",
            "--- Chunk 0 (Length: 923 chars) ---\n",
            "International Journal of Women's Dermatology xxx (2016) xxx–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Dermatology\r\n",
            " Clinical trials in dermatology\r\n",
            " K. Torre, M. Shahriari⁎\r\n",
            " Department of Dermatology, University of Connecticut Health Center, Farmington, CT\r\n",
            " art i cle info\r\n",
            " Article history:\r\n",
            " Received 13 December 2016\r\n",
            " Accepted 14 December 2016\r\n",
            " Available online xxxx\r\n",
            " Keywords:\r\n",
            " clinical trials\r\n",
            " design\r\n",
            " randomized\r\n",
            " controlled\r\n",
            " placebo\r\n",
            " double...\n",
            "\n",
            "--- Chunk 1 (Length: 996 chars) ---\n",
            "videtreatmentsthatnot onlyofferobjectiveimprovementsinclinicaldiseasestatus butalsosubjective im\r\n",
            "provementsinthequalityoflifeofpatients whoare afflictedwith thedisease.Inthisarticle, wediscussthe\r\n",
            " processes andresourcesofaclinicaltrialsunitandthe challengesthatcanbeencounteredduringthestudy\r\n",
            " process. It is critical to engage in clinical trials to treat patients most effectively with new and innovative\r\n",
            " therapies that are rooted in trial-validated, evidence-based medicine.\r\n",
            " ©2016Women'sDerma...\n",
            "\n",
            "--- Last 2 Chunk(s) ---\n",
            "\n",
            "--- Chunk 29 (Length: 972 chars) ---\n",
            "Prey S, Paul C, Bronsard V. Assessment of the risk of psoriatic arthritis in patients with\r\n",
            " plaque psoriasis: a systematic review of the literature. J Eur Acad Dermatol\r\n",
            " Venereol 2010;24:31–5.\r\n",
            " Rapp SR, Feldman SR, Exum ML, Fleischer Jr AB, Reboussin DM. Psoriasis causes as\r\n",
            " much disability as other major medical diseases. J Am Acad Dermatol 1999;41:\r\n",
            " 401–7.\r\n",
            " RossS, GrantA,Counsell C,Gillespie W, RussellI, Prescott R. Barriers to participation in\r\n",
            " randomised controlled trials: a systemati...\n",
            "\n",
            "--- Chunk 30 (Length: 543 chars) ---\n",
            "Weng X, Liu L, Barcellos LF, Allison JE, Herrinton LJ. Clustering of inflammatory\r\n",
            " bowel disease with immune mediated diseases among members of a\r\n",
            " northern California-managed care organization. Am J Gastroenterol 2007;102:\r\n",
            " 1429–35.\r\n",
            " Zarbin MA.Challenges inapplying the results of clinical trials to clinical practice. JAMA\r\n",
            " Ophthalmol 2016;134:928–33.\r\n",
            " Please cite this article as: Torre K, Shahriari M, Clinical trials in dermatology, International Journal of Women's Dermatology (2016), http...\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- Query (Find Longest/Shortest): Which chunks are the longest and shortest? ---\n",
            "Shortest Chunk: Index 30, Length 543 chars\n",
            "Content (first 300 chars): Weng X, Liu L, Barcellos LF, Allison JE, Herrinton LJ. Clustering of inflammatory\r\n",
            " bowel disease with immune mediated diseases among members of a\r\n",
            " northern California-managed care organization. Am J Gastroenterol 2007;102:\r\n",
            " 1429–35.\r\n",
            " Zarbin MA.Challenges inapplying the results of clinical trials...\n",
            "--------------------\n",
            "Longest Chunk: Index 27, Length 997 chars\n",
            "Content (first 300 chars): Gerbens LA, Prinsen CA, Chalmers JR, Drucker AM, von Kobyletzki LB, Limpens J, et al.\r\n",
            " Evaluation of the measurementproperties ofsymptommeasurementinstruments\r\n",
            " for atopic eczema: a systematic review. Allergy 2017;72:146–63.\r\n",
            " Green L. Explaining the role of the nurse in clinical trials. Nurs Stand...\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- Query (Simple Concat Summary): Create a simple summary from chunk beginnings ---\n",
            "Combined Summary (approx 1000 chars):\n",
            "International Journal of Women's Dermatology xxx (2016) xxx–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Dermatology\r\n",
            " Clinical trials in dermatology\r\n",
            " K. Torre ... videtreatmentsthatnot onlyofferobjectiveimprovementsinclinicaldiseasestatus butalsosubjective im\r\n",
            "provementsinthequalityoflifeofpatients whoare afflictedwith thedisease.Inthisarticle, wediscussthe\r\n",
            " p ... King Nebuchadnezzar, a military leader in Babylon, that instructs\r\n",
            " people to eat a diet of meat and wine only, which he believed\r\n",
            " wouldkeepthemphysicallyfitandhealthy(Collier, 2009).However,\r\n",
            " agrou ... prevent scurvy (Bartholomew, 2002). By 1863, another element of\r\n",
            " modern clinical trial protocols, the placebo, was added to research\r\n",
            " studies (de Craen et al., 1999). In the1900s, more advanced conc ... conduct for human experimentation. The United States also imple\r\n",
            "mented its own laws and regulatory organizations to govern clinical\r\n",
            " trial research; however, many years, legislative acts, and unfort\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if chunking was performed\n",
        "if 'chunked_results' not in locals() or not document_text:\n",
        "    print(\"Chunking results not found. Please run the chunking cell first.\")\n",
        "else:\n",
        "    for strategy_name, result in chunked_results.items():\n",
        "        print(f\"\\n--- Inspecting {strategy_name} Chunks ---\")\n",
        "        if 'error' in result:\n",
        "            print(f\"  Error: {result['error']}\")\n",
        "        else:\n",
        "            print(f\"  Total Chunks: {result['num_chunks']}\")\n",
        "            docs = result['chunks']\n",
        "            if docs:\n",
        "                # Show details of first and last chunk\n",
        "                first_doc = docs[0]\n",
        "                last_doc = docs[-1]\n",
        "                print(f\"  First Chunk Length (chars): {len(first_doc.page_content)}\")\n",
        "                print(f\"  First Chunk Preview: {first_doc.page_content[:150]}...\")\n",
        "                print(f\"  Last Chunk Length (chars): {len(last_doc.page_content)}\")\n",
        "                print(f\"  Last Chunk Preview: {last_doc.page_content[:150]}...\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hLyYjbhePfE",
        "outputId": "bc5d8669-f7d4-4fa0-d4bd-234bdbf410dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting Fixed Size Chunks ---\n",
            "  Total Chunks: 1\n",
            "  First Chunk Length (chars): 27282\n",
            "  First Chunk Preview: International Journal of Women's Dermatology xxx (2016) xxx–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Derma...\n",
            "  Last Chunk Length (chars): 27282\n",
            "  Last Chunk Preview: International Journal of Women's Dermatology xxx (2016) xxx–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Derma...\n",
            "\n",
            "--- Inspecting Sentence Chunks ---\n",
            "  Total Chunks: 18\n",
            "  First Chunk Length (chars): 1568\n",
            "  First Chunk Preview: international journal of women ' s dermatology xxx ( 2016 ) xxx – xxx contents lists available at sciencedirect international journal of women ' s der...\n",
            "  Last Chunk Length (chars): 298\n",
            "  Last Chunk Preview: ##ing the results of clinical trials to clinical practice. jama ophthalmol 2016 ; 134 : 928 – 33. please cite this article as : torre k, shahriari m, ...\n",
            "\n",
            "--- Inspecting Recursive Chunks ---\n",
            "  Total Chunks: 31\n",
            "  First Chunk Length (chars): 923\n",
            "  First Chunk Preview: International Journal of Women's Dermatology xxx (2016) xxx–xxx\r\n",
            "\r\n",
            " Contents lists available at ScienceDirect\r\n",
            " International Journal of Women's Derma...\n",
            "  Last Chunk Length (chars): 543\n",
            "  Last Chunk Preview: Weng X, Liu L, Barcellos LF, Allison JE, Herrinton LJ. Clustering of inflammatory\r\n",
            " bowel disease with immune mediated diseases among members of a\r\n",
            " n...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Day-6\n",
        "  Introduction to embeddings"
      ],
      "metadata": {
        "id": "QxLyoj0Kk8S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Select two chunking methods to work with ---\n",
        "# You can change these names if you prefer different methods\n",
        "selected_method_1 = \"Recursive\" # Example: \"Fixed Size\", \"Recursive\", \"Sentence\"\n",
        "selected_method_2 = \"Sentence\"  # Example: \"Fixed Size\", \"Recursive\", \"Sentence\"\n",
        "\n",
        "print(f\"Selected Chunking Methods for Embedding:\")\n",
        "print(f\"1. {selected_method_1}\")\n",
        "print(f\"2. {selected_method_2}\")\n",
        "\n",
        "# Check if the selected methods were processed successfully\n",
        "if selected_method_1 not in chunked_results or selected_method_2 not in chunked_results:\n",
        "    print(\" Error: One or both selected methods were not found in chunked_results.\")\n",
        "    print(\"Please check the chunking step or adjust the selected method names above.\")\n",
        "else:\n",
        "    if 'error' in chunked_results[selected_method_1]:\n",
        "        print(f\"Error with {selected_method_1}: {chunked_results[selected_method_1]['error']}\")\n",
        "    if 'error' in chunked_results[selected_method_2]:\n",
        "        print(f\"Error with {selected_method_2}: {chunked_results[selected_method_2]['error']}\")\n",
        "\n",
        "    # Proceed only if both methods are okay\n",
        "    if ('error' not in chunked_results[selected_method_1]) and ('error' not in chunked_results[selected_method_2]):\n",
        "        print(\" Both selected methods are ready for embedding.\")\n",
        "        # Extract the chunked documents (LangChain Document objects)\n",
        "        chunks_method_1 = chunked_results[selected_method_1]['chunks']\n",
        "        chunks_method_2 = chunked_results[selected_method_2]['chunks']\n",
        "\n",
        "        # Extract the actual text content from the LangChain Document objects\n",
        "        texts_method_1 = [doc.page_content for doc in chunks_method_1]\n",
        "        texts_method_2 = [doc.page_content for doc in chunks_method_2]\n",
        "\n",
        "        print(f\" Extracted text chunks for {selected_method_1}: {len(texts_method_1)} chunks\")\n",
        "        print(f\" Extracted text chunks for {selected_method_2}: {len(texts_method_2)} chunks\")\n",
        "\n",
        "    else:\n",
        "        print(\" Cannot proceed due to errors in selected chunking methods.\")\n",
        "        # Stop or handle error appropriately\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxcj6I3zeQLU",
        "outputId": "40022e4b-fd07-4492-ebba-ea5fae218a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Chunking Methods for Embedding:\n",
            "1. Recursive\n",
            "2. Sentence\n",
            " Both selected methods are ready for embedding.\n",
            " Extracted text chunks for Recursive: 31 chunks\n",
            " Extracted text chunks for Sentence: 18 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install the SentenceTransformer library ---\n",
        "# This library is needed to generate embeddings\n",
        "!pip install -q sentence-transformers # -q for quiet installation\n",
        "print(\" sentence-transformers library installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW8DXOCZkewp",
        "outputId": "f4bc1d8b-c8fe-46a0-e7dd-ce2e04ee2ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " sentence-transformers library installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Import SentenceTransformer ---\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Load a pre-trained SentenceTransformer model ---\n",
        "# You can choose different models. 'all-MiniLM-L6-v2' is a good, fast, and effective general-purpose model.\n",
        "# Others: 'all-mpnet-base-v2' (more powerful but slower), 'paraphrase-MiniLM-L6-v2', etc.\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "print(f\"Loading SentenceTransformer model: {model_name}...\")\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "print(\" Model loaded successfully.\")\n",
        "\n",
        "# --- Generate Embeddings for Selected Methods ---\n",
        "embeddings_method_1 = None\n",
        "embeddings_method_2 = None\n",
        "\n",
        "try:\n",
        "    print(f\"Generating embeddings for {selected_method_1} chunks...\")\n",
        "    embeddings_method_1 = embedding_model.encode(texts_method_1)\n",
        "    print(f\" Embeddings generated for {selected_method_1}. Shape: {embeddings_method_1.shape}\")\n",
        "\n",
        "    print(f\"Generating embeddings for {selected_method_2} chunks...\")\n",
        "    embeddings_method_2 = embedding_model.encode(texts_method_2)\n",
        "    print(f\"Embeddings generated for {selected_method_2}. Shape: {embeddings_method_2.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error generating embeddings: {e}\")\n",
        "\n",
        "# --- (Optional) Inspect Embeddings ---\n",
        "# You can now use these embeddings for similarity search, clustering, etc.\n",
        "if embeddings_method_1 is not None:\n",
        "    print(f\"\\n--- Sample Embedding for {selected_method_1} (First Chunk) ---\")\n",
        "    print(f\"Type: {type(embeddings_method_1[0])}\")\n",
        "    print(f\"Length: {len(embeddings_method_1[0])}\")\n",
        "    # Print first 10 values as an example\n",
        "    print(f\"First 10 values: {embeddings_method_1[0][:10]}\")\n",
        "\n",
        "if embeddings_method_2 is not None:\n",
        "    print(f\"\\n--- Sample Embedding for {selected_method_2} (First Chunk) ---\")\n",
        "    print(f\"Type: {type(embeddings_method_2[0])}\")\n",
        "    print(f\"Length: {len(embeddings_method_2[0])}\")\n",
        "    # Print first 10 values as an example\n",
        "    print(f\"First 10 values: {embeddings_method_2[0][:10]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXEzJdGnkmKm",
        "outputId": "0bb81b30-9dec-4862-92cb-a494e766dee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SentenceTransformer model: all-MiniLM-L6-v2...\n",
            " Model loaded successfully.\n",
            "Generating embeddings for Recursive chunks...\n",
            " Embeddings generated for Recursive. Shape: (31, 384)\n",
            "Generating embeddings for Sentence chunks...\n",
            "Embeddings generated for Sentence. Shape: (18, 384)\n",
            "\n",
            "--- Sample Embedding for Recursive (First Chunk) ---\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Length: 384\n",
            "First 10 values: [-0.01096862 -0.04836176  0.02251827  0.06169205  0.06097835  0.01381819\n",
            "  0.01888117  0.12289523 -0.05065198  0.02655983]\n",
            "\n",
            "--- Sample Embedding for Sentence (First Chunk) ---\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Length: 384\n",
            "First 10 values: [-0.0233663  -0.056681    0.04592495  0.06573939  0.04685322  0.01565016\n",
            " -0.00477829  0.1276771  -0.06525842  0.02469622]\n"
          ]
        }
      ]
    }
  ]
}